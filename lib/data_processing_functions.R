# data_processing_functions
# --
# This set of functions is useful for importing and filtering raw variant call data.
# --
# /// Author --- SIMON FELIX BRUNNER
# /// Creation date --- 12-JAN-2018

library(dplyr)
library(data.table)

process_bulkconverted <- function(bulkconvert_tsv_path, outpath, par_CLPM=0, par_ASMD=140, skip=66, subs_only=T, targeted=F) {
  # Imports tsv files generated by the CASM bulkconvert.pl script and generates an output
  # file that is useful for LCM data analysis.
  #
  # /// Author --- SIMON FELIX BRUNNER
  # /// Creation date --- 12-JAN-2018
  #
  # Args
  #   bulkconvert_tsv_path: path of bulkconvert tsv file
  #   outpath: where to save output table
  #   par_CLPM: only variants below or equal to this CLPM threshold will be returned
  #   par_ASMD: only variants above or equal to this ASMD threshold will be returned
  #   skip: lines to skip during fread of bulkconvert file
  #   subs_only: if TRUE, indels are removed from output
  #   targeted: if TRUE, then treats data as targeted (ie. no ASMD, CLPM)
  #   
  # Returns:
  #   tibble of variants
  # )
  
  print(sprintf('Importing variants from path %s', bulkconvert_tsv_path))
  print(sprintf('Filter parameters are %d and %d', par_ASMD, par_CLPM))
  
  # Import raw data
  caveman_bulk = tbl_df(fread(bulkconvert_tsv_path, header = T, skip=skip))
  
  # Get coverage
  #caveman_bulk = mutate(caveman_bulk, coverage = as.numeric(`FAZ-Tum`) + as.numeric(`FCZ-Tum`) + as.numeric(`FGZ-Tum`) + as.numeric(`FTZ-Tum`) + 
  #                      as.numeric(`RAZ-Tum`) + as.numeric(`RCZ-Tum`) + as.numeric(`RGZ-Tum`) + as.numeric(`RTZ-Tum`))
  caveman_bulk$coverage = rowSums(matrix(as.numeric(as.matrix(caveman_bulk[,c('FAZ-Tum', 'FCZ-Tum', 'FGZ-Tum', 'FTZ-Tum', 'RAZ-Tum', 'RCZ-Tum', 'RGZ-Tum', 'RTZ-Tum')], ncol = 8)), ncol=8), na.rm=TRUE)
  
  # Select columns of interest
  if(targeted) {
    caveman_bulk$ASMD = par_ASMD
    caveman_bulk$CLPM = par_CLPM
  }
  caveman_tbl = dplyr::select(caveman_bulk, chrom=Chrom, pos=Pos, sample=Sample, normal=Normal, ref=Ref, alt=Alt, vaf=`PM-Tum`, 
                              gene=Gene, type=Type, effect=Effect, DS, SNP, ASMD, CLPM, filter=Filter, coverage)
  
  # Calculate variant IDs
  caveman_tbl = mutate(caveman_tbl, pos_id = paste(sample, chrom, pos, sep='_'))
  caveman_tbl = mutate(caveman_tbl, mut_id = paste(chrom, pos, ref, alt, sep='_'))
  
  # Filter for subs
  if(subs_only) {
    caveman_tbl = filter(caveman_tbl, type=='Sub') 
  }
  
  # Assign donor sample
  caveman_tbl = mutate(caveman_tbl, donor = factor(substr(sample, 1, 8)))
  
  # Determine the groupsize (number of samples sharing same variant)
  caveman_tbl = group_by(caveman_tbl, donor, mut_id) %>%
    mutate(groupsize = n()) %>% ungroup()
  
  # Apply ASMD / CLPM filters
  if(!is.na(par_CLPM)) {
    caveman_tbl_filt = filter(caveman_tbl, CLPM<=par_CLPM)
  } else {
    caveman_tbl_filt = caveman_tbl
  }
  if(!is.na(par_ASMD)) {
    caveman_tbl_filt = filter(caveman_tbl_filt, ASMD>=par_ASMD)
  }
  
  if(targeted) { caveman_tbl_filt = caveman_tbl_filt %>% dplyr::select(-CLPM, -ASMD) }
  
  fwrite(caveman_tbl_filt, file = outpath)
  
  return(caveman_tbl_filt)
}

process_bulkconverted_pindel <- function(bulkconvert_tsv_path, outpath, skip=73) {
  # Imports tsv files of Pindel variants generated by the CASM bulkconvert.pl script and generates an output
  # file that is useful for LCM data analysis.
  #
  # /// Author --- SIMON FELIX BRUNNER
  # /// Creation date --- 14-MAY-2018
  #
  # Args
  #   bulkconvert_tsv_path: path of bulkconvert tsv file
  #   outpath: where to save output table
  #   skip: lines to skip during fread of bulkconvert file
  #   
  # Returns:
  #   tibble of variants
  # )
  
  print(sprintf('Importing variants from path %s', bulkconvert_tsv_path))
  
  # Import raw data
  pindel_bulk = tbl_df(fread(bulkconvert_tsv_path, header = T, skip=skip))
  
  # Get coverage
  pindel_bulk = mutate(pindel_bulk, coverage = `PR-Tum` + `NR-Tum`)
  
  # Get relevant columns
  pindel_bulk = dplyr::select(pindel_bulk, chrom=Chrom, pos=Pos, sample=Sample, normal=Normal, ref=Ref, alt=Alt, vaf=`VAF-Tum`, 
                              gene=Gene, type=Type, effect=Effect, filter=Filter, qual=Qual, coverage, coverage_alt=`MTR-Tum`, F017, F018=`FLG-F018`, S1, S2, LEN, REP)
  
  # Calculate variant IDs
  pindel_bulk = mutate(pindel_bulk, pos_id = paste(sample, chrom, pos, sep='_'))
  pindel_bulk = mutate(pindel_bulk, mut_id = paste(chrom, pos, ref, alt, sep='_'))
  
  # Assign donor sample
  pindel_bulk = mutate(pindel_bulk, donor = factor(substr(sample, 1, 8)))
  
  # Determine the groupsize (number of samples sharing same variant)
  pindel_bulk = group_by(pindel_bulk, donor, mut_id) %>%
    mutate(groupsize = n()) %>% ungroup()
  
  fwrite(pindel_bulk, file = outpath)
  
  return(pindel_bulk)
}

extract_sample_name <- function(filename) {
  if(grepl('lo', filename)) {
    sample_name = paste(strsplit(filename, '_')[[1]][1:2], collapse='_')
  } else {
    sample_name = strsplit(filename, '_')[[1]][1]
  }
  return(sample_name)
}

process_mathijs <- function(caveman_vars_path, mathijs_dir, outpath, PASS_only=T) {
  # Imports VCF files generated by Mathijs Sanders' filter steps, merge them with Caveman variants.
  #
  # /// Author --- SIMON FELIX BRUNNER
  # /// Creation date --- 12-JAN-2018
  #
  # Args
  #   caveman_vars_path: path to csv file containing all Caveman variants
  #   mathijs_dir: directory containing Mathijs' VCF files
  #   outpath: path to write merged variants
  #   PASS_only: if TRUE, then only passing variants are returned
  #   
  # Returns:
  #   tibble of variants

  # Load Caveman variants
  caveman_tbl = tbl_df(fread(caveman_vars_path, header=T))
  unique_samples = unique(caveman_tbl$sample)
  
  # Retrieve list of Mathijs filter files
  vcf_files = tbl_df(list.files(mathijs_dir))
  vcf_files = filter(vcf_files, grepl('final_4.vcf', value))
  
  vcf_lst = list()
  
  # Load each VCF file
  for(i in c(1:length(vcf_files$value))) {
    #sample_name = substr(unlist(strsplit(vcf_files$value[i],'[.]'))[1],1,14)
    sample_name = extract_sample_name(vcf_files$value[i])
    if(sample_name %in% unique_samples) {
      fpath = file.path(mathijs_dir,vcf_files$value[i])
      print(sprintf('Loading VCF from path %s', fpath))
      
      param = ScanVcfParam(info = c(''), geno = c(''))
      shearwater_vcf = load_vcf_with_variantannotation(fpath = fpath, va_params = param)
      if(dim(shearwater_vcf)[1]==0) { next }
      shearwater_vcf$sample = sample_name
      shearwater_vcf = mutate(shearwater_vcf, merge_id = paste(sample, chrom, start, ref, alt, sep='_'))
      vcf_lst[[i]] = shearwater_vcf
    }
  }
  
  vcf_tbl = do.call('rbind',vcf_lst)
  
  caveman_tbl_mathijs = left_join(mutate(caveman_tbl, merge_id = paste(sample, chrom, pos, ref, alt, sep='_')), 
                                  dplyr::select(vcf_tbl, merge_id, mathijs_filt = filt), by='merge_id')
  caveman_tbl_mathijs = mutate(caveman_tbl_mathijs, mathijs_filt = ifelse(is.na(mathijs_filt), 'FAIL', mathijs_filt)) %>% 
    dplyr::select(-merge_id)
  
  if(PASS_only) {
    caveman_tbl_mathijs = filter(caveman_tbl_mathijs, mathijs_filt=='PASS')
  }
  
  # Write to disk
  fwrite(caveman_tbl_mathijs, file = outpath)
  
  return(caveman_tbl_mathijs)
}

process_cgpvaf <- function(cgpvaf_tsv_path, skip=85) {
  # Imports tsv files generated by the CASM cgpvaf script and generates multiple
  # output tibbles listing of depth, ref counts, alt counts, and vafs
  #
  # /// Author --- SIMON FELIX BRUNNER
  # /// Creation date --- 15-MAR-2018
  #
  # Args
  #   cgpvaf_tsv_path: path of cgpvaf tsv file
  #   skip: lines to skip during fread of bulkconvert file
  #   
  # Returns:
  #   list(
  #     depth_tbl, # each entry is the total depth (_DEP entries)
  #     alt_tbl,   # each entry is the number of alt reads (_MTR entries)
  #     ref_tbl,   # each entry is the number of ref reads (_WTR entries)
  #     vaf_tbl    # each entry is the vaf (_VAF entries)
  # )
  
  # Importing raw table
  vaf_tbl_raw = tbl_df(fread(cgpvaf_tsv_path, skip=skip)) %>%
    rename(chrom=Chrom, pos=Pos, normal=`#Normal`, ref=Ref, alt=Alt)
  
  # Define features to keep
  fixed_feats = c('chrom', 'pos', 'normal', 'ref', 'alt')
  
  keep_feats = c(fixed_feats, 
                 names(vaf_tbl_raw)[grepl('DEP', names(vaf_tbl_raw))], 
                 names(vaf_tbl_raw)[grepl('MTR', names(vaf_tbl_raw))], 
                 names(vaf_tbl_raw)[grepl('WTR', names(vaf_tbl_raw))], 
                 names(vaf_tbl_raw)[grepl('VAF', names(vaf_tbl_raw))])
  
  # Gather the table and filter down to relevant variables
  vaf_tbl_gathered = vaf_tbl_raw[,keep_feats] %>% 
    gather(key='varname', value='varval', keep_feats[(length(fixed_feats)+1):length(keep_feats)]) %>% 
    mutate(sample = str_replace(varname, '_DEP', '')) %>%
    mutate(sample = str_replace(sample, '_MTR', '')) %>%
    mutate(sample = str_replace(sample, '_WTR', '')) %>%
    mutate(sample = str_replace(sample, '_VAF', '')) %>%
    mutate(varname = ifelse(grepl('DEP', varname), 'depth', 
                            ifelse(grepl('MTR', varname), 'alt_count', 
                                   ifelse(grepl('WTR', varname), 'ref_count', 
                                          ifelse(grepl('VAF', varname), 'vaf', varname))))) %>%
    dplyr::select(chrom, pos, sample, normal, ref, alt, varname, varval)
  
  # Generate the return tables
  depth_tbl = vaf_tbl_gathered %>% 
    filter(varname == 'depth') %>%
    spread(key='sample', value='varval')
  
  alt_tbl = vaf_tbl_gathered %>% 
    filter(varname == 'alt_count') %>%
    spread(key='sample', value='varval')
  
  ref_tbl = vaf_tbl_gathered %>% 
    filter(varname == 'ref_count') %>%
    spread(key='sample', value='varval')
  
  vaf_tbl = vaf_tbl_gathered %>% 
    filter(varname == 'vaf') %>%
    spread(key='sample', value='varval')
  
  return(list(depth_tbl, alt_tbl, ref_tbl, vaf_tbl))
}